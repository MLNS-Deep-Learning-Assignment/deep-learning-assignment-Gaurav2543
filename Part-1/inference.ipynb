{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data files: 100%|██████████| 6/6 [00:00<00:00, 55.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 30000 test images\n",
      "Using device: cuda\n",
      "\n",
      "=== Evaluating Model ===\n",
      "Loading model from best_digit_sum_cnn_batch64_lr0.0001.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 938/938 [00:10<00:00, 87.12it/s, Batch Accuracy=0.312] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating metrics...\n",
      "\n",
      "=== Results ===\n",
      "accuracy: 0.5065\n",
      "mae: 0.6142\n",
      "rmse: 0.9729\n",
      "within_one_accuracy: 0.9112\n",
      "mean_confidence: 0.3464\n",
      "correct_confidence: 0.3530\n",
      "incorrect_confidence: 0.3397\n",
      "\n",
      "=== Generating Visualizations ===\n",
      "Generating confusion matrix...\n",
      "Saved confusion matrix to confusion_matrix.png\n",
      "Generating error distribution plot...\n",
      "Saved error distribution to error_dist.png\n",
      "Visualizing 10 sample predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Plotting samples: 100%|██████████| 10/10 [00:00<00:00, 1043.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved prediction visualization to sample_predictions.png\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, mean_absolute_error, mean_squared_error\n",
    "\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self, num_classes=37):  # 0 to 36 inclusive\n",
    "        super().__init__()\n",
    "        \n",
    "        # Feature extraction layers\n",
    "        self.features = nn.Sequential(\n",
    "            # First conv block\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout2d(0.25),\n",
    "            \n",
    "            # Second conv block\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout2d(0.25),\n",
    "            \n",
    "            # Third conv block\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout2d(0.25)\n",
    "        )\n",
    "        \n",
    "        # Calculate size of flattened features\n",
    "        self._to_linear = self._get_conv_output_size((1, 40, 168))\n",
    "        \n",
    "        # Classification layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self._to_linear, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _get_conv_output_size(self, shape):\n",
    "        \"\"\"Calculate the size of flattened features after convolutions\"\"\"\n",
    "        batch_size = 1\n",
    "        input = torch.autograd.Variable(torch.rand(batch_size, *shape))\n",
    "        output_feat = self.features(input)\n",
    "        n_size = output_feat.data.view(batch_size, -1).size(1)\n",
    "        return n_size\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize model weights\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "def preprocess_image(image):\n",
    "    \"\"\"Preprocess a single image for inference\"\"\"\n",
    "    if len(image.shape) == 2:\n",
    "        image = image.reshape(1, 40, 168)\n",
    "    \n",
    "    # Convert to torch tensor if numpy array\n",
    "    if isinstance(image, np.ndarray):\n",
    "        image = torch.FloatTensor(image)\n",
    "    \n",
    "    # Keep as single channel and normalize\n",
    "    model_input = image.reshape(-1, 1, 40, 168)\n",
    "    model_input = (model_input - model_input.mean()) / model_input.std()\n",
    "    \n",
    "    return model_input\n",
    "\n",
    "def load_model(model_path, device='cuda'):\n",
    "    \"\"\"Load a trained model from path\"\"\"\n",
    "    print(f\"Loading model from {model_path}...\")\n",
    "    model = CustomCNN().to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "class ModelEvaluator:\n",
    "    def __init__(self, model_path, device='cuda'):\n",
    "        self.device = device\n",
    "        self.model = load_model(model_path, device)\n",
    "        self.model.eval()\n",
    "    \n",
    "    def evaluate_batch(self, images, labels, batch_size=32):\n",
    "        \"\"\"Evaluate a batch of images\"\"\"\n",
    "        predictions = []\n",
    "        confidences = []\n",
    "        \n",
    "        # Create progress bar for batch processing\n",
    "        num_batches = (len(images) + batch_size - 1) // batch_size\n",
    "        batch_iterator = tqdm(range(0, len(images), batch_size), \n",
    "                            total=num_batches,\n",
    "                            desc=\"Processing batches\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in batch_iterator:\n",
    "                batch_images = images[i:i + batch_size]\n",
    "                \n",
    "                # Preprocess batch with progress tracking\n",
    "                model_inputs = []\n",
    "                for img in batch_images:\n",
    "                    model_input = preprocess_image(img)[0]\n",
    "                    model_inputs.append(model_input)\n",
    "                \n",
    "                model_input = torch.stack(model_inputs).to(self.device)\n",
    "                \n",
    "                # Get predictions\n",
    "                outputs = self.model(model_input)\n",
    "                probabilities = torch.softmax(outputs, dim=1)\n",
    "                \n",
    "                pred_probs, predicted = probabilities.max(1)\n",
    "                \n",
    "                predictions.extend(predicted.cpu().numpy())\n",
    "                confidences.extend(pred_probs.cpu().numpy())\n",
    "                \n",
    "                # Update progress bar with current metrics\n",
    "                batch_acc = (predicted.cpu().numpy() == labels[i:i + batch_size]).mean()\n",
    "                batch_iterator.set_postfix({'Batch Accuracy': f'{batch_acc:.3f}'})\n",
    "        \n",
    "        return {\n",
    "            'predictions': np.array(predictions),\n",
    "            'confidences': np.array(confidences),\n",
    "            'labels': labels\n",
    "        }\n",
    "    \n",
    "    def calculate_metrics(self, results):\n",
    "        \"\"\"Calculate evaluation metrics\"\"\"\n",
    "        predictions = results['predictions']\n",
    "        labels = results['labels']\n",
    "        confidences = results['confidences']\n",
    "        \n",
    "        print(\"Calculating metrics...\")\n",
    "        metrics = {}\n",
    "        \n",
    "        # Basic metrics\n",
    "        metrics['accuracy'] = (predictions == labels).mean()\n",
    "        metrics['mae'] = mean_absolute_error(labels, predictions)\n",
    "        metrics['rmse'] = np.sqrt(mean_squared_error(labels, predictions))\n",
    "        \n",
    "        # Within-1 accuracy\n",
    "        within_one = np.abs(predictions - labels) <= 1\n",
    "        metrics['within_one_accuracy'] = within_one.mean()\n",
    "        \n",
    "        # Confidence analysis\n",
    "        metrics['mean_confidence'] = confidences.mean()\n",
    "        metrics['correct_confidence'] = confidences[predictions == labels].mean()\n",
    "        metrics['incorrect_confidence'] = confidences[predictions != labels].mean() if any(predictions != labels) else 0\n",
    "        \n",
    "        # Error distribution\n",
    "        metrics['error_distribution'] = predictions - labels\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def plot_confusion_matrix(self, results, save_path=None):\n",
    "        \"\"\"Plot confusion matrix\"\"\"\n",
    "        print(\"Generating confusion matrix...\")\n",
    "        plt.figure(figsize=(15, 15))\n",
    "        cm = confusion_matrix(results['labels'], results['predictions'])\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        if save_path:\n",
    "            plt.savefig(save_path)\n",
    "            plt.close()\n",
    "            print(f\"Saved confusion matrix to {save_path}\")\n",
    "        else:\n",
    "            plt.show()\n",
    "    \n",
    "    def plot_error_distribution(self, metrics, save_path=None):\n",
    "        \"\"\"Plot error distribution\"\"\"\n",
    "        print(\"Generating error distribution plot...\")\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.hist(metrics['error_distribution'], bins=range(-10, 11), align='left')\n",
    "        plt.title('Error Distribution')\n",
    "        plt.xlabel('Prediction Error (Predicted - True)')\n",
    "        plt.ylabel('Count')\n",
    "        if save_path:\n",
    "            plt.savefig(save_path)\n",
    "            plt.close()\n",
    "            print(f\"Saved error distribution to {save_path}\")\n",
    "        else:\n",
    "            plt.show()\n",
    "    \n",
    "    def visualize_predictions(self, images, results, num_samples=10, save_path=None):\n",
    "        \"\"\"Visualize sample predictions\"\"\"\n",
    "        print(f\"Visualizing {num_samples} sample predictions...\")\n",
    "        predictions = results['predictions']\n",
    "        labels = results['labels']\n",
    "        confidences = results['confidences']\n",
    "        \n",
    "        # Randomly select samples\n",
    "        indices = np.random.choice(len(images), num_samples, replace=False)\n",
    "        \n",
    "        # Create figure\n",
    "        fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
    "        axes = axes.ravel()\n",
    "        \n",
    "        for idx, ax in enumerate(tqdm(axes, desc=\"Plotting samples\")):\n",
    "            if idx < num_samples:\n",
    "                img = images[indices[idx]]\n",
    "                pred = predictions[indices[idx]]\n",
    "                true = labels[indices[idx]]\n",
    "                conf = confidences[indices[idx]]\n",
    "                \n",
    "                # Display image\n",
    "                ax.imshow(img.reshape(40, 168), cmap='gray')\n",
    "                color = 'green' if pred == true else 'red'\n",
    "                ax.set_title(f'Pred: {pred} (True: {true})\\nConf: {conf:.2f}', \n",
    "                           color=color)\n",
    "                ax.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        if save_path:\n",
    "            plt.savefig(save_path)\n",
    "            plt.close()\n",
    "            print(f\"Saved prediction visualization to {save_path}\")\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "def main():\n",
    "    print(\"Loading test data...\")\n",
    "    data_path = '/scratch/gaurav.bhole/MLNS_data/'\n",
    "    \n",
    "    # Load data files with progress tracking\n",
    "    data_files = {\n",
    "        'data0': 'data0.npy',\n",
    "        'data1': 'data1.npy',\n",
    "        'data2': 'data2.npy',\n",
    "        'lab0': 'lab0.npy',\n",
    "        'lab1': 'lab1.npy',\n",
    "        'lab2': 'lab2.npy'\n",
    "    }\n",
    "    \n",
    "    loaded_data = {}\n",
    "    for name, filename in tqdm(data_files.items(), desc=\"Loading data files\"):\n",
    "        loaded_data[name] = np.load(data_path + filename)\n",
    "    \n",
    "    # Combine the data\n",
    "    test_images = np.concatenate(\n",
    "        (loaded_data['data0'], loaded_data['data1'], loaded_data['data2']), \n",
    "        axis=0\n",
    "    )\n",
    "    test_labels = np.concatenate(\n",
    "        (loaded_data['lab0'], loaded_data['lab1'], loaded_data['lab2']), \n",
    "        axis=0\n",
    "    )\n",
    "    print(f\"Loaded {len(test_images)} test images\")\n",
    "\n",
    "    # Path to trained model\n",
    "    model_path = 'best_digit_sum_cnn_batch64_lr0.0001.pt'\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Create evaluator and run evaluation\n",
    "    print(\"\\n=== Evaluating Model ===\")\n",
    "    evaluator = ModelEvaluator(model_path, device)\n",
    "    results = evaluator.evaluate_batch(test_images, test_labels)\n",
    "    metrics = evaluator.calculate_metrics(results)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(\"\\n=== Results ===\")\n",
    "    for metric, value in metrics.items():\n",
    "        if metric != 'error_distribution':\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "    # Generate visualizations\n",
    "    print(\"\\n=== Generating Visualizations ===\")\n",
    "    evaluator.plot_confusion_matrix(results, save_path='confusion_matrix.png')\n",
    "    evaluator.plot_error_distribution(metrics, save_path='error_dist.png')\n",
    "    evaluator.visualize_predictions(test_images, results, save_path='sample_predictions.png')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
